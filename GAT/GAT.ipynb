{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAT 图神经网络学习笔记\n",
    "\n",
    "_关于图、GAT 的学习记录_\n",
    "\n",
    "_为何叫 GAT 捏，因为 GAN 一般指的是 Generative Adversal Nets_\n",
    "\n",
    "## 数据结构-图\n",
    "\n",
    "图的三个特征\n",
    "\n",
    "- `node` 节点，每个顶点有着自己的特征，node2vec 将顶点的特征转换为一个高维向量 $h_i$\n",
    "- `edge` 边，节点之间的连接\n",
    "- `global` 图，包含着全局特征\n",
    "\n",
    "下面这个图就是一个无向连接的图 $G$ ，其有五个节点，每个节点有**相邻节点**和此节点自身的**特征（可以是一个数值、向量、矩阵）**  \n",
    "![](https://pic1.zhimg.com/80/v2-ec415ca61d7eef27296aff1994e91db8_1440w.webp)\n",
    "\n",
    "可写出图 G 的邻接矩阵 A ：\n",
    "\n",
    "![](./img/邻接矩阵.png)\n",
    "\n",
    "## inductive 和 transductive\n",
    "\n",
    "[如何理解 inductive learning 与 transductive learning?](https://www.zhihu.com/question/68275921)  \n",
    "区别在于**预测的样本**是否在我们训练的时候已经用过  \n",
    "`transductive` 直推式学习 训练阶段和测试阶段都基于同样的图结构  \n",
    "`inductive` 归纳式学习 处理动态图 训练阶段和测试阶段要处理的图不同\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## GAT 架构\n",
    "\n",
    "**Graph Attention Network**\n",
    "\n",
    "有着两种计算方式  \n",
    "`Global Graph Attention` 节点 i 与图上所有节点都做 attention 计算  \n",
    "`Mask Graph Attention` 节点 i 只与邻居节点做 attention 计算， GAT 使用的是这个方法\n",
    "\n",
    "### Graph Attentional Layer\n",
    "\n",
    "GAT 架构通过堆叠图注意力层来实现  \n",
    "先来看注意力系数(attention coefficients)的计算：  \n",
    "$$e_{ij} = a(W\\vec{h_i}, W\\vec{h_j})$$  \n",
    "eij 表示节点 j 的特征对节点 i 的影响\n",
    "\n",
    "下面为 GAL 的核心公式：  \n",
    "$$a_{ij} = softmax(\\sigma(\\vec{a}^T[W\\vec{h_i}||W\\vec{h_j}]))$$\n",
    "_其中 || 为 concatenate，表示张量的粘合，比如[[1， 2], [3，4]]粘合[[5, 6], [7, 8]]变成[[1， 2], [3，4]，[5, 6], [7, 8]]_\n",
    "\n",
    "- $\\vec{h_i}$和$\\vec{h_j}$表示张量的节点 i 和 j 的节点特征，维度是 1 x $F$\n",
    "- $W$是权重矩阵，维度$F'$ x $F$ ，这个权重矩阵是共享的，可以应用于每一个节点\n",
    "- $\\vec{a}^T$ 为 attention kernal, 维度为$2F'$ x 1\n",
    "- $\\sigma$ 激活函数用的是 LeakyReLu(负斜率=0.2)  \n",
    "  关于激活函数可以阅读[激活函数(Sigmoid/ReLU/LeakyReLU/PReLU/ELU)](https://zhuanlan.zhihu.com/p/172254089)这篇文章\n",
    "\n",
    "观察这些维度，会惊奇的发现经过公式的一顿操作之后的结果是一个实数 R, 这个数就是 attention 系数，表明 j 节点对 i 节点的重要程度\n",
    "\n",
    "这时候再看论文里的这个图系不系了然于心 😉\n",
    "\n",
    "![](./img/图注意力机制.png)\n",
    "\n",
    "了解了$a_{ij}$的计算公式，再来看这个公式  \n",
    "$$ \\vec{h_i}' = \\sigma(\\sum*{j\\in{N*i}}a*{ij}W\\vec{h*j})$$\n",
    "\n",
    "- $\\vec{h_i}'$ 表示这层 GAL 关于节点 i 的输出特征\n",
    "- $N_i$ 表示节点 i 的邻接节点\n",
    "- $a*{ij}$ 表示上述公式的结果即注意力系数\n",
    "- 这里的$\\sigma$激活函数采用的是 ELU\n",
    "\n",
    "### multi-head attention\n",
    "\n",
    "先看这个图\n",
    "\n",
    "![](./img/多头注意力.png)\n",
    "\n",
    "图上 3 条不同颜色的线就代表着 3 个独立的 attention 系数\n",
    "$$\\vec{h_i}' = \\parallel_{K=1}^K\\sigma(\\sum_{j\\in{N_i}}a_{ij}^kW^k\\vec{h_j})$$\n",
    "上面公式代表中间层的输出形式\n",
    "最后一层即预测层采取加权平方法，  \n",
    "下面的公式为输出层形式：\n",
    "$$\\vec{h_i}' = \\sigma(\\frac{1}{K}\\sum_{k=1}^K\\sum_{j\\in{N_i}}a_{ij}^kW^k\\vec{h_j})$$\n",
    "输出层的$\\sigma$用的是 softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代码实现\n",
    "\n",
    "_用的是 keras_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for head in range(self.attn_heads):\n",
    "    kernel = self.kernels[head]  # W in the paper (F x F')\n",
    "    # Attention kernel a in the paper (2F' x 1)\n",
    "    attention_kernel = self.attn_kernels[head]\n",
    "    # Compute inputs to attention network\n",
    "    features = K.dot(X, kernel)  # (N x F')\n",
    "\n",
    "    # Compute feature combinations\n",
    "    # Note: [[a_1], [a_2]]^T [[Wh_i], [Wh_2]] = [a_1]^T [Wh_i] + [a_2]^T [Wh_j]\n",
    "    # (N x 1), [a_1]^T [Wh_i]\n",
    "    attn_for_self = K.dot(features, attention_kernel[0])\n",
    "    # (N x 1), [a_2]^T [Wh_j]\n",
    "    attn_for_neighs = K.dot(features, attention_kernel[1])\n",
    "\n",
    "    # Attention head a(Wh_i, Wh_j) = a^T [[Wh_i], [Wh_j]]\n",
    "    # (N x N) via broadcasting\n",
    "    dense = attn_for_self + K.transpose(attn_for_neighs)\n",
    "\n",
    "    # Add nonlinearty\n",
    "    dense = LeakyReLU(alpha=0.2)(dense)\n",
    "\n",
    "    # Mask values before activation (Vaswani et al., 2017)\n",
    "    mask = -10e9 * (1.0 - A)\n",
    "    dense += mask\n",
    "\n",
    "    # Apply softmax to get attention coefficients\n",
    "    dense = K.softmax(dense)  # (N x N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAT 的优点\n",
    "\n",
    "1. 计算速度快， 可以在不同节点上进行并行运算\n",
    "2. 可以同时对拥有不同度的节点进行处理\n",
    "3. 可以被直接用于解决归纳学习(inductive)问题，即可以对从未见过的图结构进行处理\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 参考\n",
    "\n",
    "[【GNN】图注意力网络 GAT](https://zhuanlan.zhihu.com/p/112938037)  \n",
    "[keras 实现 GAT](https://github.com/danielegrattarola/keras-gat)  \n",
    "[pytorch 实现 GAT](https://github.com/Diego999/pyGAT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 英语学习\n",
    "\n",
    "### 专业类词汇\n",
    "\n",
    "inductive 归纳  \n",
    "transductive 转导  \n",
    "benchmark 基准  \n",
    "recursive 递归的、循环的  \n",
    "grid-like 网格状  \n",
    "acyclic 非周期、非环状  \n",
    "cyclic  \n",
    "sepctral  \n",
    "aggregated 聚集  \n",
    "explicit 显式的  \n",
    "coefficient 系数\n",
    "\n",
    "### 描述类\n",
    "\n",
    "leverage 使用、利用  \n",
    "arbitrarily  \n",
    "intense computation 密集计算\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
