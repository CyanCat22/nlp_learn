{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT\n",
    "\n",
    "**B**idirection **E**ncoder **R**epresentation from **T**ransformer\n",
    "\n",
    "本文是笔者对 Bert 经典论文的学习、以及写作技巧的学习笔记  \n",
    "小小萌新，有错误、意见欢迎提出 😆\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 标题作者\n",
    "2. 摘要\n",
    "3. 导言\n",
    "4. 结论\n",
    "5. 相关工作\n",
    "6. BERT 模型细节\n",
    "7. 实验\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "双向性、预训练、微调\n",
    "机器翻译、摘要、生成效果不太好\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于基础知识、前提概要也要进行简要的解释，便于大家理解\n",
    "例如本文的 pre-training 和 fine-tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert\n",
    "\n",
    "### 迁移学习\n",
    "\n",
    "- 使用预训练好的模型来抽取词、句子特征（例如 word2vec 或语言模型）\n",
    "- 不更新训练好的模型\n",
    "- 需要构建新的网络来抓取新任务需要的信息（word2vec 忽略了时序信息，语言模型只看了一个方向）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "\n",
    "1. BERT 分为哪两种任务，各自的作用是什么；\n",
    "2. 在计算 MLM 预训练任务的损失函数的时候，参与计算的 Tokens 有哪些？是全部的 15%的词汇还是 15%词汇中真正被 Mask 的那些 tokens？\n",
    "3. 在实现损失函数的时候，怎么确保没有被 Mask 的函数不参与到损失计算中去；\n",
    "4. BERT 的三个 Embedding 为什么直接相加\n",
    "5. BERT 的优缺点分别是什么？\n",
    "6. 你知道有哪些针对 BERT 的缺点做优化的模型？\n",
    "7. BERT 怎么用在生成模型中？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 如何找研究想法？\n",
    "\n",
    "## 打补丁法\n",
    "\n",
    "eg:\n",
    "将想法荟萃在一起，考验写作能力，要有故事性\n",
    "\n",
    "- MAE 基于 ViT + BERT\n",
    "  - 遮住更多的图片块\n",
    "    - 编码时只处理没遮住的\n",
    "  - 用 Transformer 来输出（解码）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 引用\n",
    "\n",
    "[BERT 论文]()\n",
    "[]()\n",
    "[]()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
